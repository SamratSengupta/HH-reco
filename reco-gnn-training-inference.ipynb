{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3246b822-8fe0-46ba-a033-281678884899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samra\\AppData\\Local\\Temp\\ipykernel_11372\\198322853.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  edges['PERSON_ID'] = edges['PERSON_ID'].map(person_id_map)\n",
      "C:\\Users\\samra\\AppData\\Local\\Temp\\ipykernel_11372\\198322853.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  edges['ProviderCode'] = edges['ProviderCode'].map(provider_id_map)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Load the interaction data\n",
    "data_path = 'metadata/'\n",
    "interaction_df = pd.read_csv(data_path + 'interactions.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Preprocess the interaction data\n",
    "interaction_df['SurveyAnswerScore'] = np.round(interaction_df['SurveyAnswerScore'], 2)\n",
    "interaction_df['RecommendStar'] = np.round(interaction_df['RecommendStar'], 2)\n",
    "interaction_df = interaction_df.drop('InsertedOn', axis=1)\n",
    "\n",
    "# Aggregate the interaction data\n",
    "interaction_df_mod = interaction_df.groupby(['PERSON_ID', 'ProviderCode']).agg({\n",
    "    'SurveyAnswerScore': 'mean',\n",
    "    'RecommendStar': 'mean',\n",
    "    'num_visits': 'first',\n",
    "    'age': 'mean',\n",
    "    'ratings': 'mean',\n",
    "    'star_rating': 'first',\n",
    "    'survey_score': 'first',\n",
    "    'zip_code': 'first',\n",
    "    'GENDER_Female': 'first',\n",
    "    'GENDER_Male': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "interaction_df_mod['visit_ratings'] = 0.65 * interaction_df_mod['ratings'] + 0.35 * interaction_df_mod['num_visits']\n",
    "\n",
    "# Prepare provider features\n",
    "provider_features = interaction_df_mod.groupby('ProviderCode').agg({\n",
    "    'num_visits': 'sum',\n",
    "    'survey_score': 'mean',\n",
    "    'star_rating': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Prepare person features\n",
    "person_features = interaction_df_mod.groupby('PERSON_ID').agg({\n",
    "    'age': 'first',\n",
    "    'GENDER_Female': 'first',\n",
    "    'GENDER_Male': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Prepare edge features\n",
    "edges = interaction_df_mod[['PERSON_ID', 'ProviderCode', 'visit_ratings']]\n",
    "\n",
    "# Map IDs to integers\n",
    "person_id_map = {id: idx for idx, id in enumerate(person_features['PERSON_ID'].unique())}\n",
    "provider_id_map = {id: idx + len(person_id_map) for idx, id in enumerate(provider_features['ProviderCode'].unique())}\n",
    "id_provider_map =  {value: key for key, value in provider_id_map.items()}\n",
    "\n",
    "# Apply mapping to the edges\n",
    "edges['PERSON_ID'] = edges['PERSON_ID'].map(person_id_map)\n",
    "edges['ProviderCode'] = edges['ProviderCode'].map(provider_id_map)\n",
    "\n",
    "# Initialize the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add person nodes with features\n",
    "for _, row in person_features.iterrows():\n",
    "    G.add_node(person_id_map[row['PERSON_ID']], bipartite=0, age=row['age'], gender_female=row['GENDER_Female'], gender_male=row['GENDER_Male'])\n",
    "\n",
    "# Add provider nodes with features\n",
    "for _, row in provider_features.iterrows():\n",
    "    G.add_node(provider_id_map[row['ProviderCode']], bipartite=1, num_visits=row['num_visits'], survey_score=row['survey_score'], star_rating=row['star_rating'])\n",
    "\n",
    "# Add edges with features\n",
    "for _, row in edges.iterrows():\n",
    "    G.add_edge(row['PERSON_ID'], row['ProviderCode'], visit_ratings=row['visit_ratings'])\n",
    "\n",
    "# Convert to adjacency matrix\n",
    "adj_matrix = nx.to_numpy_array(G)\n",
    "adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)\n",
    "\n",
    "# Extract node features for persons and providers\n",
    "person_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 0]\n",
    "provider_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 1]\n",
    "\n",
    "person_features = torch.tensor([\n",
    "    [d['age'], d['gender_female'], d['gender_male']] \n",
    "    for n, d in G.nodes(data=True) if n in person_nodes\n",
    "], dtype=torch.float32)\n",
    "\n",
    "provider_features = torch.tensor([\n",
    "    [d['num_visits'], d['survey_score'], d['star_rating']]\n",
    "    for n, d in G.nodes(data=True) if n in provider_nodes\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Combine person and provider features into a single feature matrix\n",
    "all_features = []\n",
    "for node in G.nodes(data=True):\n",
    "    if node[1]['bipartite'] == 0:\n",
    "        all_features.append([node[1]['age'], node[1]['gender_female'], node[1]['gender_male']])\n",
    "    else:\n",
    "        all_features.append([node[1]['num_visits'], node[1]['survey_score'], node[1]['star_rating']])\n",
    "\n",
    "features = torch.tensor(all_features, dtype=torch.float32)\n",
    "\n",
    "# Prepare edge indices and labels\n",
    "edge_list = np.array(G.edges)\n",
    "labels = np.array([G[u][v]['visit_ratings'] for u, v in G.edges])\n",
    "\n",
    "# Split the edges into train, validation, and test sets\n",
    "train_edges, test_edges, train_labels, test_labels = train_test_split(edge_list, labels, test_size=0.2, random_state=42)\n",
    "train_edges, val_edges, train_labels, val_labels = train_test_split(train_edges, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_edge_index = torch.tensor(train_edges, dtype=torch.long)\n",
    "val_edge_index = torch.tensor(val_edges, dtype=torch.long)\n",
    "test_edge_index = torch.tensor(test_edges, dtype=torch.long)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32).view(-1, 1)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.float32).view(-1, 1)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f435ee9a-ff9a-40d1-9f76-e57b0be12a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.8, l2_reg=1e-5):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "        self.l2_reg = l2_reg\n",
    "    \n",
    "    def forward(self, adjacency_matrix, feature_matrix):\n",
    "        h = torch.mm(adjacency_matrix, feature_matrix)\n",
    "        h = self.linear(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.batch_norm(h)\n",
    "        return F.relu(h)\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, dropout=0.8, l2_reg=1e-5):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.gnn1 = GNNLayer(in_features, hidden_features, dropout, l2_reg)\n",
    "        self.gnn2 = GNNLayer(hidden_features, out_features, dropout, l2_reg)\n",
    "    \n",
    "    def forward(self, adjacency_matrix, feature_matrix):\n",
    "        h = self.gnn1(adjacency_matrix, feature_matrix)\n",
    "        h = self.gnn2(adjacency_matrix, h)\n",
    "        return h\n",
    "\n",
    "# Assume the feature size for persons is 3 (age, gender_female, gender_male)\n",
    "# and for providers is 3 (num_visits, survey_score, star_rating)\n",
    "model = GNNModel(in_features=3, hidden_features=32, out_features=1, dropout=0.8, l2_reg=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950cbcc2-bda5-420b-90de-1585f0e12280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 9.262527465820312, Val Loss: 9.642000198364258\n",
      "Epoch 2, Train Loss: 9.47844409942627, Val Loss: 9.594399452209473\n",
      "Epoch 3, Train Loss: 8.540969848632812, Val Loss: 9.232390403747559\n",
      "Epoch 4, Train Loss: 8.13715934753418, Val Loss: 8.709370613098145\n",
      "Epoch 5, Train Loss: 7.686004638671875, Val Loss: 8.116190910339355\n",
      "Epoch 6, Train Loss: 7.26003360748291, Val Loss: 7.516577243804932\n",
      "Epoch 7, Train Loss: 7.24498176574707, Val Loss: 7.187308311462402\n",
      "Epoch 8, Train Loss: 6.826931953430176, Val Loss: 6.964295387268066\n",
      "Epoch 9, Train Loss: 6.644498348236084, Val Loss: 6.877194404602051\n",
      "Epoch 10, Train Loss: 6.639065742492676, Val Loss: 6.877134799957275\n",
      "Epoch 11, Train Loss: 6.43827486038208, Val Loss: 6.939988613128662\n",
      "Epoch 12, Train Loss: 6.2265496253967285, Val Loss: 6.879476547241211\n",
      "Epoch 13, Train Loss: 6.212185382843018, Val Loss: 6.762838840484619\n",
      "Epoch 14, Train Loss: 6.09029483795166, Val Loss: 6.700096130371094\n",
      "Epoch 15, Train Loss: 5.775910377502441, Val Loss: 6.600689888000488\n",
      "Epoch 16, Train Loss: 5.587129592895508, Val Loss: 6.475689888000488\n",
      "Epoch 17, Train Loss: 5.6275105476379395, Val Loss: 6.373707294464111\n",
      "Epoch 18, Train Loss: 5.855223178863525, Val Loss: 6.3124871253967285\n",
      "Epoch 19, Train Loss: 5.418818473815918, Val Loss: 6.155029296875\n",
      "Epoch 20, Train Loss: 5.345618724822998, Val Loss: 6.005832195281982\n",
      "Epoch 21, Train Loss: 5.598214626312256, Val Loss: 5.897209167480469\n",
      "Epoch 22, Train Loss: 5.213530540466309, Val Loss: 5.732946395874023\n",
      "Epoch 23, Train Loss: 5.076761245727539, Val Loss: 5.56274938583374\n",
      "Epoch 24, Train Loss: 4.958878993988037, Val Loss: 5.4243927001953125\n",
      "Epoch 25, Train Loss: 4.66254186630249, Val Loss: 5.284247398376465\n",
      "Epoch 26, Train Loss: 4.8547515869140625, Val Loss: 5.16109037399292\n",
      "Epoch 27, Train Loss: 4.729959964752197, Val Loss: 5.039916515350342\n",
      "Epoch 28, Train Loss: 4.701869010925293, Val Loss: 4.926342964172363\n",
      "Epoch 29, Train Loss: 4.595007419586182, Val Loss: 4.821374893188477\n",
      "Epoch 30, Train Loss: 4.590296745300293, Val Loss: 4.722243785858154\n",
      "Epoch 31, Train Loss: 4.24669885635376, Val Loss: 4.6048431396484375\n",
      "Epoch 32, Train Loss: 4.328186988830566, Val Loss: 4.4908647537231445\n",
      "Epoch 33, Train Loss: 4.169895648956299, Val Loss: 4.407535076141357\n",
      "Epoch 34, Train Loss: 4.184274196624756, Val Loss: 4.3731608390808105\n",
      "Epoch 35, Train Loss: 4.189718723297119, Val Loss: 4.33358097076416\n",
      "Epoch 36, Train Loss: 3.996577501296997, Val Loss: 4.279658317565918\n",
      "Epoch 37, Train Loss: 3.898639678955078, Val Loss: 4.256131649017334\n",
      "Epoch 38, Train Loss: 3.9099831581115723, Val Loss: 4.212438583374023\n",
      "Epoch 39, Train Loss: 3.8065812587738037, Val Loss: 4.158400535583496\n",
      "Epoch 40, Train Loss: 3.6829957962036133, Val Loss: 4.067938327789307\n",
      "Epoch 41, Train Loss: 3.617011547088623, Val Loss: 3.9944653511047363\n",
      "Epoch 42, Train Loss: 3.7217049598693848, Val Loss: 3.916642189025879\n",
      "Epoch 43, Train Loss: 3.476945638656616, Val Loss: 3.8384246826171875\n",
      "Epoch 44, Train Loss: 3.260262966156006, Val Loss: 3.748593807220459\n",
      "Epoch 45, Train Loss: 3.4373176097869873, Val Loss: 3.676969289779663\n",
      "Epoch 46, Train Loss: 3.357008218765259, Val Loss: 3.627185344696045\n",
      "Epoch 47, Train Loss: 3.2917885780334473, Val Loss: 3.5630686283111572\n",
      "Epoch 48, Train Loss: 3.263620376586914, Val Loss: 3.5046422481536865\n",
      "Epoch 49, Train Loss: 3.226933717727661, Val Loss: 3.4534151554107666\n",
      "Epoch 50, Train Loss: 3.3352558612823486, Val Loss: 3.4087929725646973\n",
      "Epoch 51, Train Loss: 2.9580860137939453, Val Loss: 3.3720312118530273\n",
      "Epoch 52, Train Loss: 2.867518424987793, Val Loss: 3.344519853591919\n",
      "Epoch 53, Train Loss: 2.937293767929077, Val Loss: 3.3287994861602783\n",
      "Epoch 54, Train Loss: 2.985369920730591, Val Loss: 3.322471857070923\n",
      "Epoch 55, Train Loss: 2.9915056228637695, Val Loss: 3.3219454288482666\n",
      "Epoch 56, Train Loss: 2.849773645401001, Val Loss: 3.2839419841766357\n",
      "Epoch 57, Train Loss: 2.7405898571014404, Val Loss: 3.2305333614349365\n",
      "Epoch 58, Train Loss: 2.690368413925171, Val Loss: 3.182434320449829\n",
      "Epoch 59, Train Loss: 2.950143575668335, Val Loss: 3.1543660163879395\n",
      "Epoch 60, Train Loss: 2.462015390396118, Val Loss: 3.0886197090148926\n",
      "Epoch 61, Train Loss: 2.756284236907959, Val Loss: 3.040956974029541\n",
      "Epoch 62, Train Loss: 2.6940083503723145, Val Loss: 2.9759762287139893\n",
      "Epoch 63, Train Loss: 2.5309364795684814, Val Loss: 2.928551197052002\n",
      "Epoch 64, Train Loss: 2.6479523181915283, Val Loss: 2.8844127655029297\n",
      "Epoch 65, Train Loss: 2.5565738677978516, Val Loss: 2.8515465259552\n",
      "Epoch 66, Train Loss: 2.4325506687164307, Val Loss: 2.7921807765960693\n",
      "Epoch 67, Train Loss: 2.4274017810821533, Val Loss: 2.7705090045928955\n",
      "Epoch 68, Train Loss: 2.333611249923706, Val Loss: 2.7333245277404785\n",
      "Epoch 69, Train Loss: 2.2964529991149902, Val Loss: 2.7190170288085938\n",
      "Epoch 70, Train Loss: 2.352773427963257, Val Loss: 2.6896021366119385\n",
      "Epoch 71, Train Loss: 2.2861485481262207, Val Loss: 2.6535913944244385\n",
      "Epoch 72, Train Loss: 2.1433544158935547, Val Loss: 2.6056673526763916\n",
      "Epoch 73, Train Loss: 2.0338029861450195, Val Loss: 2.5473105907440186\n",
      "Epoch 74, Train Loss: 2.1717309951782227, Val Loss: 2.4872615337371826\n",
      "Epoch 75, Train Loss: 2.2699508666992188, Val Loss: 2.4296865463256836\n",
      "Epoch 76, Train Loss: 2.298011541366577, Val Loss: 2.38950252532959\n",
      "Epoch 77, Train Loss: 2.0889761447906494, Val Loss: 2.341919422149658\n",
      "Epoch 78, Train Loss: 2.1736342906951904, Val Loss: 2.3051531314849854\n",
      "Epoch 79, Train Loss: 1.968254566192627, Val Loss: 2.2649660110473633\n",
      "Epoch 80, Train Loss: 1.8733006715774536, Val Loss: 2.2161266803741455\n",
      "Epoch 81, Train Loss: 2.0376505851745605, Val Loss: 2.1955654621124268\n",
      "Epoch 82, Train Loss: 1.897650957107544, Val Loss: 2.1610329151153564\n",
      "Epoch 83, Train Loss: 1.9109822511672974, Val Loss: 2.135413646697998\n",
      "Epoch 84, Train Loss: 1.9875000715255737, Val Loss: 2.126358985900879\n",
      "Epoch 85, Train Loss: 2.031754493713379, Val Loss: 2.115532875061035\n",
      "Epoch 86, Train Loss: 1.8264681100845337, Val Loss: 2.1051883697509766\n",
      "Epoch 87, Train Loss: 1.8677915334701538, Val Loss: 2.0895252227783203\n",
      "Epoch 88, Train Loss: 1.735902190208435, Val Loss: 2.0798065662384033\n",
      "Epoch 89, Train Loss: 1.8840296268463135, Val Loss: 2.083498239517212\n",
      "Epoch 90, Train Loss: 1.7123709917068481, Val Loss: 2.075155019760132\n",
      "Epoch 91, Train Loss: 1.8982130289077759, Val Loss: 2.066544771194458\n",
      "Epoch 92, Train Loss: 1.8376953601837158, Val Loss: 2.058206558227539\n",
      "Epoch 93, Train Loss: 1.7670053243637085, Val Loss: 2.0500824451446533\n",
      "Epoch 94, Train Loss: 1.7827502489089966, Val Loss: 2.0368618965148926\n",
      "Epoch 95, Train Loss: 1.621867299079895, Val Loss: 2.012347459793091\n",
      "Epoch 96, Train Loss: 1.6066265106201172, Val Loss: 1.9956086874008179\n",
      "Epoch 97, Train Loss: 1.8074761629104614, Val Loss: 1.9875730276107788\n",
      "Epoch 98, Train Loss: 1.76283860206604, Val Loss: 1.9755215644836426\n",
      "Epoch 99, Train Loss: 1.6729140281677246, Val Loss: 1.9463430643081665\n",
      "Epoch 100, Train Loss: 1.5437713861465454, Val Loss: 1.9226877689361572\n",
      "Epoch 101, Train Loss: 1.6650906801223755, Val Loss: 1.9033656120300293\n",
      "Epoch 102, Train Loss: 1.6356345415115356, Val Loss: 1.884722352027893\n",
      "Epoch 103, Train Loss: 1.6409378051757812, Val Loss: 1.8755261898040771\n",
      "Epoch 104, Train Loss: 1.5434657335281372, Val Loss: 1.8554518222808838\n",
      "Epoch 105, Train Loss: 1.5226694345474243, Val Loss: 1.8405269384384155\n",
      "Epoch 106, Train Loss: 1.658200740814209, Val Loss: 1.8363467454910278\n",
      "Epoch 107, Train Loss: 1.5787156820297241, Val Loss: 1.8317753076553345\n",
      "Epoch 108, Train Loss: 1.4510482549667358, Val Loss: 1.8224174976348877\n",
      "Epoch 109, Train Loss: 1.4386675357818604, Val Loss: 1.7965043783187866\n",
      "Epoch 110, Train Loss: 1.624328374862671, Val Loss: 1.7924528121948242\n",
      "Epoch 111, Train Loss: 1.5637696981430054, Val Loss: 1.7590537071228027\n",
      "Epoch 112, Train Loss: 1.3816455602645874, Val Loss: 1.731868863105774\n",
      "Epoch 113, Train Loss: 1.4753857851028442, Val Loss: 1.7223392724990845\n",
      "Epoch 114, Train Loss: 1.5659892559051514, Val Loss: 1.7026323080062866\n",
      "Epoch 115, Train Loss: 1.4796830415725708, Val Loss: 1.6831427812576294\n",
      "Epoch 116, Train Loss: 1.5836361646652222, Val Loss: 1.6808509826660156\n",
      "Epoch 117, Train Loss: 1.4688410758972168, Val Loss: 1.6708354949951172\n",
      "Epoch 118, Train Loss: 1.5450334548950195, Val Loss: 1.6488821506500244\n",
      "Epoch 119, Train Loss: 1.4101811647415161, Val Loss: 1.618955135345459\n",
      "Epoch 120, Train Loss: 1.354894995689392, Val Loss: 1.6022355556488037\n",
      "Epoch 121, Train Loss: 1.281862735748291, Val Loss: 1.5967395305633545\n",
      "Epoch 122, Train Loss: 1.4738223552703857, Val Loss: 1.5954968929290771\n",
      "Epoch 123, Train Loss: 1.3359391689300537, Val Loss: 1.5648093223571777\n",
      "Epoch 124, Train Loss: 1.3456122875213623, Val Loss: 1.5506678819656372\n",
      "Epoch 125, Train Loss: 1.402542233467102, Val Loss: 1.5626718997955322\n",
      "Epoch 126, Train Loss: 1.3057503700256348, Val Loss: 1.569293737411499\n",
      "Epoch 127, Train Loss: 1.3796937465667725, Val Loss: 1.5626614093780518\n",
      "Epoch 128, Train Loss: 1.4784377813339233, Val Loss: 1.5576449632644653\n",
      "Epoch 129, Train Loss: 1.3499078750610352, Val Loss: 1.538278341293335\n",
      "Epoch 130, Train Loss: 1.3742643594741821, Val Loss: 1.5276702642440796\n",
      "Epoch 131, Train Loss: 1.3958560228347778, Val Loss: 1.5424976348876953\n",
      "Epoch 132, Train Loss: 1.409522533416748, Val Loss: 1.553437352180481\n",
      "Epoch 133, Train Loss: 1.3888825178146362, Val Loss: 1.5461652278900146\n",
      "Epoch 134, Train Loss: 1.356601595878601, Val Loss: 1.548588752746582\n",
      "Epoch 135, Train Loss: 1.4811432361602783, Val Loss: 1.5638786554336548\n",
      "Epoch 136, Train Loss: 1.2812057733535767, Val Loss: 1.5565428733825684\n",
      "Epoch 137, Train Loss: 1.3387720584869385, Val Loss: 1.5583138465881348\n",
      "Epoch 138, Train Loss: 1.3937609195709229, Val Loss: 1.5300911664962769\n",
      "Epoch 139, Train Loss: 1.3138688802719116, Val Loss: 1.4953651428222656\n",
      "Epoch 140, Train Loss: 1.3520792722702026, Val Loss: 1.4844450950622559\n",
      "Epoch 141, Train Loss: 1.4754586219787598, Val Loss: 1.5243204832077026\n",
      "Epoch 142, Train Loss: 1.3104857206344604, Val Loss: 1.5791635513305664\n",
      "Epoch 143, Train Loss: 1.3260890245437622, Val Loss: 1.6030675172805786\n",
      "Epoch 144, Train Loss: 1.381880283355713, Val Loss: 1.6346807479858398\n",
      "Epoch 145, Train Loss: 1.4038547277450562, Val Loss: 1.6525781154632568\n",
      "Epoch 146, Train Loss: 1.2550684213638306, Val Loss: 1.68001127243042\n",
      "Epoch 147, Train Loss: 1.3796414136886597, Val Loss: 1.6831846237182617\n",
      "Epoch 148, Train Loss: 1.363875389099121, Val Loss: 1.6686795949935913\n",
      "Epoch 149, Train Loss: 1.32914400100708, Val Loss: 1.6576554775238037\n",
      "Epoch 150, Train Loss: 1.4886192083358765, Val Loss: 1.6549841165542603\n",
      "Epoch 151, Train Loss: 1.380655288696289, Val Loss: 1.6744120121002197\n",
      "Epoch 152, Train Loss: 1.2202900648117065, Val Loss: 1.6657975912094116\n",
      "Epoch 153, Train Loss: 1.2868547439575195, Val Loss: 1.652634620666504\n",
      "Epoch 154, Train Loss: 1.2468701601028442, Val Loss: 1.6523292064666748\n",
      "Epoch 155, Train Loss: 1.2217755317687988, Val Loss: 1.659242868423462\n",
      "Epoch 156, Train Loss: 1.3243242502212524, Val Loss: 1.648266077041626\n",
      "Epoch 157, Train Loss: 1.203904390335083, Val Loss: 1.6418081521987915\n",
      "Epoch 158, Train Loss: 1.3424896001815796, Val Loss: 1.632040023803711\n",
      "Epoch 159, Train Loss: 1.2436145544052124, Val Loss: 1.6247225999832153\n",
      "Epoch 160, Train Loss: 1.3056632280349731, Val Loss: 1.6145656108856201\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "def train_gcn(model, adj_matrix, features, train_edge_index, train_labels, val_edge_index, val_labels, epochs=200, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    patience = 20\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits = model(adj_matrix, features)\n",
    "\n",
    "        train_edge_logits = logits[train_edge_index[:, 0]] + logits[train_edge_index[:, 1]]\n",
    "        train_loss = criterion(train_edge_logits, train_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(adj_matrix, features)\n",
    "            val_edge_logits = val_logits[val_edge_index[:, 0]] + val_logits[val_edge_index[:, 1]]\n",
    "            val_loss = criterion(val_edge_logits, val_labels)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_gcn_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_gcn_model.pth'))\n",
    "    return model\n",
    "\n",
    "# Assume train_edge_index, train_labels, val_edge_index, and val_labels are already defined\n",
    "in_feats = features.shape[1]\n",
    "hidden_feats = 32\n",
    "out_feats = 1\n",
    "\n",
    "gcn_model = GNNModel(in_feats, hidden_feats, out_feats)\n",
    "gcn_model = train_gcn(gcn_model, adj_matrix, features, train_edge_index, train_labels, val_edge_index, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d0c6a7-c7cd-44a0-a7f0-f748a536a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN NDCG Score: 0.9321025407205367\n"
     ]
    }
   ],
   "source": [
    "def calculate_ndcg(model, adj_matrix, features, test_edge_index, test_labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(adj_matrix, features)\n",
    "        test_edge_logits = logits[test_edge_index[:, 0]] + logits[test_edge_index[:, 1]]\n",
    "        test_edge_logits = test_edge_logits.cpu().numpy().reshape(1, -1)\n",
    "        test_labels = test_labels.cpu().numpy().reshape(1, -1)\n",
    "        return ndcg_score(test_labels, test_edge_logits)\n",
    "\n",
    "# Assume test_edge_index and test_labels are already defined\n",
    "ndcg = calculate_ndcg(gcn_model, adj_matrix, features, test_edge_index, test_labels)\n",
    "print(f'GCN NDCG Score: {ndcg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7845855-9262-4d2e-b5ef-dd8c78f5866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn_model.state_dict(), 'models/gcn_reco.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "befba34c-d0a4-411c-808f-93b613e4192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNModel(\n",
       "  (gnn1): GNNLayer(\n",
       "    (linear): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gnn2): GNNLayer(\n",
       "    (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "    (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_model = GNNModel(in_feats, hidden_feats, out_feats)\n",
    "gcn_model.load_state_dict(torch.load('models/gcn_reco.pth'))\n",
    "gcn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c351294-8ac2-431f-95c9-e3057f7f57fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3472, 1.1778837442398071), (3567, 1.157257080078125), (3415, 1.1554570198059082), (3697, 1.1509815454483032), (3677, 1.1483124494552612)]\n",
      "Top recommendations for user 0071588ba3116840b32d9f7fcf3ce2707c5bbcc09a3960fbb5c91e1e7e8eb21d: ['GF4D9', 'XHRCW', '6I98Z', 'Y7PRQ', 'Y52M5']\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(model, user_id, k=5):\n",
    "        # Get the node ID for the user\n",
    "        if user_id not in person_id_map:\n",
    "            return []\n",
    "            \n",
    "        user_node = person_id_map[user_id]\n",
    "        # Get the logits (embeddings) for all nodes\n",
    "        with torch.no_grad():\n",
    "            logits = model(adj_matrix, features)\n",
    "        \n",
    "        # Get the embeddings for the user\n",
    "        user_embedding = logits[user_node]\n",
    "    \n",
    "        # Calculate the score for each provider\n",
    "        provider_scores = []\n",
    "        for provider_node in provider_nodes:\n",
    "            provider_embedding = logits[provider_node]\n",
    "            score = torch.dot(user_embedding, provider_embedding).item()\n",
    "            provider_scores.append((provider_node, score))\n",
    "    \n",
    "        # Sort the providers by score in descending order\n",
    "        provider_scores = sorted(provider_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        ## here we have to filter the  available provider nodes selected as per \n",
    "\n",
    "        print(provider_scores[:k])\n",
    "        # Get the top k providers\n",
    "        top_providers = [id_provider_map[provider_node] for provider_node, score in provider_scores[:k]]\n",
    "        \n",
    "        return top_providers\n",
    "\n",
    "# Example usage\n",
    "user_id = '0071588ba3116840b32d9f7fcf3ce2707c5bbcc09a3960fbb5c91e1e7e8eb21d'\n",
    "recommendations = get_recommendations(gcn_model, user_id)\n",
    "print(f\"Top recommendations for user {user_id}: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceae8c-b96c-4fb0-828c-0d1b6d62617b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
